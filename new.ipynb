{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def data_sequencer(data: torch.Tensor, sequence_len: int, device=\"cpu\"):\n",
    "    xs = []\n",
    "    ys = []\n",
    "\n",
    "    for i in range(len(data) - sequence_len):\n",
    "        xs.append(data[i: i+sequence_len])\n",
    "        ys.append(data[i+sequence_len])\n",
    "\n",
    "    xs = np.array(xs)\n",
    "    ys = np.array(ys)\n",
    "\n",
    "    xs = torch.tensor(xs, dtype=torch.float32, device=device)\n",
    "    ys = torch.tensor(ys, dtype=torch.float32, device=device)\n",
    "\n",
    "    return xs, ys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def data_fragmenter(df: pd.DataFrame, device=\"cpu\"):\n",
    "    scaler = MinMaxScaler()\n",
    "\n",
    "    df_fundamental = df[[\"Open\", \"High\", \"Low\", \"Close\", \"Volume\"]]\n",
    "    df_mavg = df[[\"EMA_12\", \"EMA_26\"]]\n",
    "    df_mi = df[[\"MACD\", \"Signal\", \"RSI\", \"CCI\"]]\n",
    "    df_adx = df[[\"ADX\"]]\n",
    "\n",
    "    # normalize\n",
    "    df_fundamental = scaler.fit_transform(df_fundamental)\n",
    "    df_mavg = scaler.fit_transform(df_mavg)\n",
    "    df_mi = scaler.fit_transform(df_mi)\n",
    "    df_adx = scaler.fit_transform(df_adx)\n",
    "\n",
    "    fundamental_x, fundamental_y = data_sequencer(df_fundamental, 30, device=device)\n",
    "    mavg_x, mavg_y = data_sequencer(df_mavg, 30, device=device)\n",
    "    mi_x, mi_y = data_sequencer(df_mi, 30, device=device)\n",
    "    adx_x, adx_y = data_sequencer(df_adx, 30, device=device)\n",
    "\n",
    "    return fundamental_x, fundamental_y, mavg_x, mavg_y, mi_x, mi_y, adx_x, adx_y\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"./stocks_data/AAPL_data.csv\")\n",
    "fundamental_x, fundamental_y, mavg_x, mavg_y, mi_x, mi_y, adx_x, adx_y = data_fragmenter(df=df, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Make LSTM that can adapt to shape of data\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class LSTMv1(nn.Module):\n",
    "    def __init__(self, input_features, hidden_features, num_layers, output_features, device=\"cpu\"):\n",
    "        super().__init__()\n",
    "        self.hidden_features = hidden_features\n",
    "        self.num_layers = num_layers\n",
    "        self.device = device\n",
    "\n",
    "        self.lstm = nn.LSTM(input_features, hidden_features, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_features, output_features)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_features, device=self.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_features, device=self.device)\n",
    "\n",
    "        out, (hn, cn) = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out)\n",
    "        return out[:, -1, :]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INPUT_FEATURES = len(X_train[0, 0, :])\n",
    "# HIDDEN_FEATURES = 30\n",
    "# NUM_LAYERS = 3\n",
    "# OUTPUT_FEATURES = len(y_train[0, :])\n",
    "\n",
    "model_fundamental = LSTMv1(len(fundamental_x[0, 0, :]), hidden_features=30, num_layers=3, output_features=len(fundamental_y[0, :]), device=device)\n",
    "model_mavg = LSTMv1(len(mavg_x[0, 0, :]), hidden_features=30, num_layers=3, output_features=len(mavg_y[0, :]), device=device)\n",
    "model_mi = LSTMv1(len(mi_x[0, 0, :]), hidden_features=30, num_layers=3, output_features=len(mi_y[0, :]), device=device)\n",
    "model_adx = LSTMv1(len(adx_x[0, 0, :]), hidden_features=30, num_layers=3, output_features=len(adx_y[0, :]), device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.MSELoss()\n",
    "loss_fn.to(device)\n",
    "optimizer_fundamental = torch.optim.Adam(params=model_fundamental.parameters(), lr=0.001)\n",
    "optimizer_mavg = torch.optim.Adam(params=model_mavg.parameters(), lr=0.001)\n",
    "optimizer_mi = torch.optim.Adam(params=model_mi.parameters(), lr=0.001)\n",
    "optimizer_adx = torch.optim.Adam(params=model_adx.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
